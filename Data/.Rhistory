bPHON
bPHON<-factor(m[4], levels= c("yes","no"),labels = c("yes","no"))
bPHON
b<-c(0,1,0,1,1)
b
typeof(b)
b<-c(1:3)
typeof(b)
as.numeric(m[4])
as.data.frame(m[4])
bPHON<-factor(m[4], levels= c("yes","no"),labels = c("yes","no"))
bPHON
as.numeric(m[4])
as.numeric(m[[4]])
bPHON<-factor(m[4], levels= c("yes","no"),labels = c("yes","no"))
bPHON
as.double(m[[4]])
bPHON<-factor(m[4], levels= c("yes","no"),labels = c("yes","no"))
bPhon
bPHON
is.double(m[4])
is.double(m[[4]])
is.numeric[m[4]]
is.numeric(m[4])
type(m[4])
typeof(m[4])
m<-Loan_Data
View(m)
is.data.frame(m)
is.data.frame(m[4])
as.character(m[4])
m<-Loan_Data
m[4][1]
?sample
dir <- Sys.getenv("BADS_Path")   #C:/Users/D059348/dev/HU/BADS
dir
dir <- Sys.getenv('BADS_Path')
#C:/Users/D059348/dev/HU/BADS
#dir <-getwd()
source(paste0(dir, "/Code/Utils.R"))
source(paste0(dir, "/Code/PlotHelper.R"))
print("tesfhsd")
#Script to install and load needed packages
source(paste0(dir, "/Code/Init.R"))
#Load Data
source(paste0(dir, "/Code/DataLoader.R"))
trainingset = getTrainigset(dir)
#Exploratory Data Analysis
#churn =1
subsetData_churn_true<-trainingset[trainingset$churn==1,]
#churn = 0
subsetData_churn_false<-trainingset[trainingset$churn==0,]
hist(trainingset$adults, main = "Number of Adults")
hist(trainingset$age1, col="blue", main = "Age first household member")
hist(trainingset$age2, col="red", add=TRUE)
legend("topright", c("First Household Member","Second Household Member"),lty=c(1,1), lwd=c(2.5,2.5),col=c("blue","red"))
numericVariables = getNumericVariables(trainingset)
categoricVariables <- trainingset[setdiff(colnames(trainingset), colnames(numericVariables))]
#plots hists of all 138 numeric variables
#plotHists(numericVariables, 5)
completeInds <- complete.cases(t(numericVariables))
completeCases = numericVariables[completeInds]
corrplot(cor(completeCases))
getAccuracy <- function(model, testSet){
pred <- predict(model, newdata=testSet, type="response")
class <- round(pred)
confustionTable <- CrossTable(testSet$churn, class, prop.c=FALSE)$t
return((confustionTable[1,1]+confustionTable[1,2])/length(y))
}
n <- 10
dataset = completeCases[1:1000,]
setSize = round(dim(dataset)[1]/n)
shuffledDataset <- dataset[sample(nrow(dataset)),]
avg=0
for (i in 1:n) {
print(i)
inds.test = ((i-1)*setSize+1):((i-1)*setSize+setSize)
inds.training = setdiff(1:nrow(dataset), inds.test)
test = shuffledDataset[inds.test,]
training = shuffledDataset[inds.training,]
lr<-glm(churn~.,data=training,family=binomial(link="logit"))
avg = avg + getAccuracy(model = lr, testSet = test)
}
#rf <- randomForest(churn~.,data=completeCases[1:1000,],ntree=500, mtry=3)
res = round(predict(rf, newdata = completeCases[1001:2000,]))
y = completeCases[1001:2000,]
CrossTable(y$churn, res, prop.c=FALSE)$t
#identify highly corelated coplete veriables (only numeric)
correlationMatrix <- cor(completeCases[,])
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
#verbose=TRUE
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# print indexes of highly correlated attributes
print(highlyCorrelated)
head(completeCases[,highlyCorrelated])
#delete highly corelated columns
cleanedDataCoplete<-completeCases[,-highlyCorrelated]
#remove highly corelated from the original data
colunmNames<-colnames(completeCases)[highlyCorrelated]
cleanedOriginalData<-trainingset[,!(names(trainingset) %in% colunmNames)]
View(numericVariables)
install.packages("extremevalues")
library(extremevalues)
getOutliers(completeCases[,1])
View(completeCases)
install.packages("mvoutlie")
outlieramount<-0
for(i in 1:length(outlier$wfinal01)){
if(outlier$wfinal01[i,]==0){
outlieramount<-(outlieramount+1)
}
}
test<-cleanedDataCoplete[,1:3]
outlier <- pcout(test, makeplot = TRUE)
library("mvoutlier")
test<-cleanedDataCoplete[,1:3]
outlier <- pcout(test, makeplot = TRUE)
outlieramount<-0
for(i in 1:length(outlier$wfinal01)){
if(outlier$wfinal01[i,]==0){
outlieramount<-(outlieramount+1)
}
}
for(i in 1:length(outlier$wfinal01)){
if(outlier$wfinal01[i]==0){
outlieramount<-(outlieramount+1)
}
}
outlieramount
outlier
outlier$wfinal
outlieramount<-0
for(i in 1:length(outlier$wfinal01)){
if(outlier$wfinal[i]<0.04){
outlieramount<-(outlieramount+1)
}
}
outlieramount<-0
for(i in 1:length(outlier$wfinal01)){
if(outlier$wfinal[i]=<0.04){
outlieramount<-(outlieramount+1)
}
}
outlieramount<-0
for(i in 1:length(outlier$wfinal)){
if(outlier$wfinal[i]<=0.04){
outlieramount<-(outlieramount+1)
}
}
test<-cleanedDataCoplete[,1:5]
outlier <- pcout(test, makeplot = TRUE)
test<-cleanedDataCoplete[,1:6]
outlier <- pcout(test, makeplot = TRUE)
test<-cleanedDataCoplete[,]
outlier <- pcout(test, makeplot = TRUE)
test<-cleanedDataCoplete[,1:4]
outlier <- pcout(test, makeplot = TRUE)
test<-cleanedDataCoplete[,1:3]
outlier <- pcout(test, makeplot = TRUE)
test<-cleanedDataCoplete[,1] #bei mehr als 3 variabeln spielt der Algorithmus nicht mehr mit
outlier <- pcout(test, makeplot = TRUE)
test<-cleanedDataCoplete[,1:3] #bei mehr als 3 variabeln spielt der Algorithmus nicht mehr mit
outlier <- pcout(test, makeplot = TRUE)
install.packages("HighDimOut")
library(HighDimOut)
test<-cleanedDataCoplete[,1:3] #bei mehr als 3 variabeln spielt der Algorithmus nicht mehr mit
Func.ABOD(test,basic=FALSE,0.1)
Func.ABOD(test,basic=FALSE,0.01)
Func.ABOD(test,basic=FALSE,0.0001)
install.packages("rrcovHD")
library(rrcovHD)
?OutlierPCOut()
OutlierPCOut(test)
suboutlier<- OutlierPCOut(test)
getOutliers(suboutlier)
getFlag(suboutlier)
outlieramount<-0
for(i in 1:length(suboutlier)){
if(getFlag(suboutlier)){
outlieramount<-(outlieramount+1)
}
}
getFlag(suboutlier)[1]
for(i in 1:length(suboutlier)){
if(getFlag(suboutlier)[i]==0){
outlieramount<-(outlieramount+1)
}
}
for(i in 1:length(suboutlier)){
if(getFlag(suboutlier)[i]==0){
outlieramount<-(outlieramount+1)
}
}
outlieramount<-0
for(i in 1:length(suboutlier)){
if(getFlag(suboutlier)[i]==0){
outlieramount<-(outlieramount+1)
}
}
getFlag(suboutlier)[1:10]
for(i in 1:length(test)){
if(getFlag(suboutlier)[i]==0){
outlieramount<-(outlieramount+1)
}
}
for(i in 1:50000){
if(getFlag(suboutlier)[i]==0){
outlieramount<-(outlieramount+1)
}
}
length(getOutliers(suboutlier))
outlierweight<-getWeight(suboutlier)
outlierweight
suboutlier2<- OutlierPCOut(cleanedDataCoplete)
suboutlier2<- OutlierPCOut(cleanedDataCoplete[1:5])
install.packages("outliers")
library(outliers)
outlier(test[,1])
test<-cleanedDataCoplete[,1:3]
Func.ABOD(test,basic=FALSE, 0.01)
library(HighDimOut)
Func.ABOD(test,basic=FALSE, 0.01)
View(categoricVariables)
library(HighDimOut)
test<-cleanedDataCoplete[1:20,1:38]
tim1<-Func.ABOD(test,basic=FALSE, .9)
test<-cleanedDataCoplete[1:20,1:25]
tim1<-Func.ABOD(test,basic=FALSE, .9)
library(HighDimOut)
test<-cleanedDataCoplete[1:20,1:25]
test<-cleanedDataCoplete[1:20,1:39]
tim2<- Func.FBOD(test, iter=10, k.nn=5) # cannot allocate a vector of 3.5 Gb
plot(tim2)
test<-cleanedDataCoplete[1:40,1:39]
tim2<- Func.FBOD(test, iter=10, k.nn=37) # cannot allocate a vector of 3.5 Gb
plot(tim2)
tim2<- Func.FBOD(test, iter=10, k.nn=5) # cannot allocate a vector of 3.5 Gb
plot(tim2)
tim2<- Func.FBOD(test, iter=10, k.nn=37) # cannot allocate a vector of 3.5 Gb
plot(tim2)
tim2==max(tim2)*0.25
tim2>=max(tim2)*0.75
test<-cleanedDataCoplete[1:100,1:37]
tim2<- Func.FBOD(test, iter=10, k.nn=37) # cannot allocate a vector of 3.5 Gb
plot(tim2)
tim2>=max(tim2)*0.75 # TRUE/FALSE Vector which shows all
tim2>=2 # TRUE/FALSE Vector which shows all
show.outlier<-tim2>=2 # TRUE/FALSE Vector which shows all the outliers (value 2 is manuel, lets discuss this)
rownames(show.outlier[1])
tim2(show.outlier)
tim2
tim2[show.outlier]
show.outlier<-tim2[tim2>=2] # TRUE/FALSE Vector which shows all the outliers (value 2 is manuel, lets discuss this)
show.outlier
test[tim2>=2]
test[tim2>=2,]
rownames((tim2>=2)==TRUE)
as.integer(rownames((tim2>=2)==TRUE))
as.integer((tim2>=2)==TRUE)
show.outlier
which((tim2>=2)==TRUE)
show.outlier[[1]]
for(i in 1:length(show.outlier)){
print("Potential outlier at row "; show.outlier[i])
}
row.index<-which((tim2>=2)==TRUE)
for(i in 1:length(row.index)){
paste("Potential outlier at row", row.index[i], sep = " ")
}
length(row.index)
row.index[1]
for(i in 1:length(row.index)){
paste("Potential outlier at row", row.index[i], sep = " ")
}
dir <- Sys.getenv('BADS_Path')
#C:/Users/D059348/dev/HU/BADS
#dir <-getwd()
source(paste0(dir, "/Code/Utils.R"))
source(paste0(dir, "/Code/PlotHelper.R"))
print("tesfhsd")
#Script to install and load needed packages
source(paste0(dir, "/Code/Init.R"))
#Load Data
source(paste0(dir, "/Code/DataLoader.R"))
trainingset = getTrainigset(dir)
#Exploratory Data Analysis
#churn =1
subsetData_churn_true<-trainingset[trainingset$churn==1,]
#churn = 0
subsetData_churn_false<-trainingset[trainingset$churn==0,]
hist(trainingset$adults, main = "Number of Adults")
hist(trainingset$age1, col="blue", main = "Age first household member")
hist(trainingset$age2, col="red", add=TRUE)
legend("topright", c("First Household Member","Second Household Member"),lty=c(1,1), lwd=c(2.5,2.5),col=c("blue","red"))
numericVariables = getNumericVariables(trainingset)
categoricVariables <- trainingset[setdiff(colnames(trainingset), colnames(numericVariables))]
#plots hists of all 138 numeric variables
#plotHists(numericVariables, 5)
completeInds <- complete.cases(t(numericVariables))
completeCases = numericVariables[completeInds]
corrplot(cor(completeCases))
getAccuracy <- function(model, testSet){
pred <- predict(model, newdata=testSet, type="response")
class <- round(pred)
confustionTable <- CrossTable(testSet$churn, class, prop.c=FALSE)$t
return((confustionTable[1,1]+confustionTable[1,2])/length(y))
}
n <- 10
dataset = completeCases[1:1000,]
setSize = round(dim(dataset)[1]/n)
shuffledDataset <- dataset[sample(nrow(dataset)),]
avg=0
for (i in 1:n) {
print(i)
inds.test = ((i-1)*setSize+1):((i-1)*setSize+setSize)
inds.training = setdiff(1:nrow(dataset), inds.test)
test = shuffledDataset[inds.test,]
training = shuffledDataset[inds.training,]
lr<-glm(churn~.,data=training,family=binomial(link="logit"))
avg = avg + getAccuracy(model = lr, testSet = test)
}
#rf <- randomForest(churn~.,data=completeCases[1:1000,],ntree=500, mtry=3)
res = round(predict(rf, newdata = completeCases[1001:2000,]))
y = completeCases[1001:2000,]
CrossTable(y$churn, res, prop.c=FALSE)$t
#Corelation
#identify highly corelated coplete veriables (only numeric)
correlationMatrix <- cor(completeCases[,])
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
#verbose=TRUE
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.75)
# print indexes of highly correlated attributes
print(highlyCorrelated)
head(completeCases[,highlyCorrelated])
#delete highly corelated columns
cleanedDataCoplete<-completeCases[,-highlyCorrelated]
#remove highly corelated from the original data
colunmNames<-colnames(completeCases)[highlyCorrelated]
cleanedOriginalData<-trainingset[,!(names(trainingset) %in% colunmNames)]
library(HighDimOut)
test<-cleanedDataCoplete[1:100,1:37]
tim2<- Func.FBOD(test, iter=10, k.nn=37) # cannot allocate a vector of 3.5 Gb
#tim is calculated with LOF
# value <<1 the point is in the cluster; value >> the point is far away from cluster
plot(tim2)
#tim2 shows much better solutions than tim1!!!
#WHY? Berechnet mit LOF, zwar nicht mit Angel; dennoch stärker für die Anwendung (erster Eindruck)
show.outlier<-tim2[tim2>=2] # TRUE/FALSE Vector which shows all the outliers (value 2 is manuel, lets discuss this)
test[tim2>=2,]
row.index<-which((tim2>=2)==TRUE)
row.index
plot(tim2)
plot(tim2)
plot(tim2)
colnames(trainingset[,1])
variable.names(trainingset[,1])
labels(trainingset[,1])
labels(trainingset[1,])
labels(trainingset)
labels(trainingset) [[2]]
labels(trainingset) [[2]] [1]
labels(trainingset) [[2]] [1] == "rev_Mean"
labels(trainingset) [[2]] [1] == "REV_MEAN"
labels(trainingset) [[2]] [1] == "rev_MeaN"
labels(trainingset) [[2]] [1] == "rev_Mean"
tolower(labels(trainingset) [[2]] [1]) == tolower("rev_Mean")
tolower(labels(trainingset) [[2]] [1]) == tolower("REV_MEAN")
contin.var <- list("ADJMOU", "REV_MEAN", "TORSTEN")
contin.var[1]
contin.var[2]
contin.var[3]
contin.var[[3]]
z.scale <- function(v){
m<- mean(v)
s<- sd(v)
for(i in 1:length(v)){
v[i]<-((v[i]-m)/s)
}
return(v)
}
test <- trainingset[1]
test
labels(test)
z.scale.data <- function(m,continous.var){
for(i in 1:length(m)){
for(k in 1:length(continous.var)){
if(tolower((labels(m)[[2]][i]))==tolower(continous.var[[k]])){
m[,i]<-z.scale(m[,i])
}
}
}
}
length(test)
length(contin.var)
test[1]
labels(test[1])
labels(test[[1]])
labels(test[[2]])
labels(test[1])
test$rev_Mean
test$rev_Mean[1:5]
z.scale.data(test,contin.var)
test$rev_Mean[1:5]
tolower((labels(test)[[2]][1]))
tolower((labels(test)[[2]][1])) == tolower(contin.var[[2]])
length(test)
length(trainingset)
length(contin.var)
tolower(contin.var[[2]])
z.scale(test[,1][1:5])
test2 <- test[,1] [1:5]
test2
test2[,1][1:5]<-z.scale(test[,1][1:5])
test2[,1]<-z.scale(test[,1][1:5])
test2<-test
test2[,1]<-z.scale(test[,1])
test2[,1][1:5]
z.scale(test[,1])
test<-trainingsset[1:3]
test<-trainingset[1:3]
test2<-trainingset[1:3]
z.scale(test[,1][1:5])
test2 <- z.scale.data(m=test,continous.var = contin.var)
test[,1] [1:5]
z.scale.data <- function(m,continous.var){
for(i in 1:length(m)){
for(k in 1:length(continous.var)){
if(tolower((labels(m)[[2]][i]))==tolower(continous.var[[k]])){
m[,i]<-z.scale(m[,i])
}
}
}
return(m)
}
test2<-test
z.scale.data(test,contin.var)
z.scale(test2[,1] [1:5])
z.scale(m[,1])
z.scale(test2[,1])
test2[1:5]
test2[,1][1:5]
z.scale(test2[,1])
test2[,1] [1:5]
length(test2[,1] [1:5])
z.scale(test2[,1][1:5])
z.scale(test2[,1])
z.scale(test2[,1][1:20])
setwd("C:/Users/Max/Desktop/BADS")
contin.var <- list((read.csv("continous_variablenames",header=TRUE, sep = ";")))
contin.var <- list((read.csv("continous_variablenames.csv",header=TRUE, sep = ";")))
setwd("C:/Users/Max/Desktop/BADS/Data")
contin.var <- list((read.csv("continous_variablenames.csv",header=TRUE, sep = ";")))
contin.var[1]
contin.var[,1]
contin.var[[1]]
t(contin.var)
contin.var <- list((read.csv("continous_variablenames.csv",header=TRUE, sep = ";")))
contin.var[1:5]
contin.var[1]
length(contin.var)
ncol(contin.var)
ncol(contin.var)
nrow(contin.var)
contin.var <- list((read.csv("continous_variablenames.csv",header=TRUE, sep = ";")))
length(contin.var)
nrow(contin.var)
contin.var <- data.frame((read.csv("continous_variablenames.csv",header=TRUE, sep = ";")))
length(contin.var)
nrow(contin.var)
contin.var[1]
contin.var[1,]
tolower(contin.var[1,])
test<-trainingsset[1:3]
test<-trainingset[1:3]
test[1:5]
test[1:5,]
tolower(labels(test)[[2]][1])
tolower((labels(m)[[2]][i])) == tolower(contin.var[1,])
tolower((labels(test)[[2]][1])) == tolower(contin.var[1,])
tolower((labels(test)[[2]][1])) == tolower(contin.var[96,])
z.scale.data(m=test[1:5,],continous.var = contin.var)
z.scale.data <- function(m,continous.var){
e <- m
for(i in 1:length(m)){
for(k in 1:nrow(continous.var)){
if(tolower((labels(m)[[2]][i]))==tolower(continous.var[k,])){
m[,i]<-z.scale(m[,i])
}
}
}
return(m)
}
z.scale <- function(v){#funtioniert nicht bei hochen samples?
m<- mean(v)
s<- sd(v)
for(i in 1:length(v)){
v[i]<-((v[i]-m)/s)
}
return(v)
}
z.scale.data(m=test[1:5,],continous.var = contin.var)
gescaled <- z.scale.data(m=test[1:5,],continous.var = contin.var)
gescaled
gescaled [1,1]
data.frame()
?"data.frame
?data.frame
mean(test[,1])
mean(test[,1][1:1000])
mean(test[,1][1:100])
